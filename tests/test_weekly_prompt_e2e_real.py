"""E2E tests with real LLM API calls using DeepSeek.

These tests call real LLM API and incur costs.
Set RUN_REAL_LLM_TESTS=true to enable.
"""

import os
import pytest
from datetime import datetime
from typing import List, Dict, Any
import httpx

pytestmark = [
    pytest.mark.asyncio,
    pytest.mark.skipif(
        os.getenv("RUN_REAL_LLM_TESTS") != "true",
        reason="Real LLM tests disabled. Set RUN_REAL_LLM_TESTS=true to enable."
    ),
]


class DeepSeekClient:
    """Simple DeepSeek API client for testing."""
    
    def __init__(self):
        self.api_key = os.getenv("TEST_LLM_API_KEY")
        self.model = os.getenv("TEST_LLM_MODEL", "deepseek-chat")
        self.base_url = "https://api.deepseek.com/v1"
        
        if not self.api_key:
            raise RuntimeError("TEST_LLM_API_KEY not set")
    
    async def chat(self, messages: List[Dict[str, str]], system_prompt: str = None) -> str:
        """Send chat request to DeepSeek."""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        full_messages = []
        if system_prompt:
            full_messages.append({"role": "system", "content": system_prompt})
        full_messages.extend(messages)
        
        payload = {
            "model": self.model,
            "messages": full_messages,
            "temperature": 0.7,
            "max_tokens": 500
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload
            )
            response.raise_for_status()
            data = response.json()
            return data["choices"][0]["message"]["content"]


@pytest.fixture
async def deepseek_client():
    """Provide DeepSeek client."""
    return DeepSeekClient()


class TestRealPromptInjection:
    """Test real LLM behavior with weekly system prompts."""
    
    async def test_prompt_changes_llm_behavior(self, deepseek_client):
        """Verify system prompt truly changes LLM response style."""
        question = "ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯å˜é‡"
        
        # Baseline: no system prompt
        baseline_response = await deepseek_client.chat(
            [{"role": "user", "content": question}]
        )
        
        # With system prompt forcing kitchen metaphor
        prompt_response = await deepseek_client.chat(
            [{"role": "user", "content": question}],
            system_prompt="ä½ å¿…é¡»ç”¨å¨æˆ¿çƒ¹é¥ªçš„æ¯”å–»æ¥è§£é‡Šæ‰€æœ‰ç¼–ç¨‹æ¦‚å¿µ"
        )
        
        # Verify style changed (should contain kitchen-related words)
        kitchen_keywords = ["å¨æˆ¿", "çƒ¹é¥ª", "é”…", "èœ", "åšé¥­", "é£Ÿæ", "è°ƒæ–™"]
        has_kitchen_metaphor = any(kw in prompt_response for kw in kitchen_keywords)
        
        assert has_kitchen_metaphor, (
            f"Expected kitchen metaphor in response, got: {prompt_response}"
        )
        assert baseline_response != prompt_response
    
    async def test_teaching_style_constraint(self, deepseek_client):
        """Test that system prompt enforces teaching style."""
        question = "Python çš„åˆ—è¡¨å’Œå…ƒç»„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
        
        # Strict teaching style: no direct answers, use questions
        strict_prompt = """ä½ æ˜¯ä¸¥æ ¼çš„è‹æ ¼æ‹‰åº•å¼å¯¼å¸ˆã€‚
è§„åˆ™ï¼š
1. ä¸ç›´æ¥ç»™å‡ºç­”æ¡ˆ
2. å¿…é¡»ç”¨æé—®å¼•å¯¼å­¦ç”Ÿæ€è€ƒ
3. æ¯ä¸ªå›ç­”è‡³å°‘åŒ…å«3ä¸ªé—®é¢˜"""
        
        response = await deepseek_client.chat(
            [{"role": "user", "content": question}],
            system_prompt=strict_prompt
        )
        
        # Verify Socratic style (multiple questions)
        question_count = response.count("?") + response.count("ï¼Ÿ")
        assert question_count >= 3, (
            f"Expected at least 3 questions, found {question_count} in: {response}"
        )
    
    async def test_language_constraint(self, deepseek_client):
        """Test that system prompt enforces output language."""
        question = "What is a function in programming?"
        
        # Force Chinese response even for English question
        chinese_prompt = "ç”¨æˆ·ç”¨ä»»ä½•è¯­è¨€æé—®ï¼Œä½ éƒ½å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ã€‚"
        
        response = await deepseek_client.chat(
            [{"role": "user", "content": question}],
            system_prompt=chinese_prompt
        )
        
        # Verify response is in Chinese (contains Chinese characters)
        chinese_chars = sum('\u4e00' <= c <= '\u9fff' for c in response)
        assert chinese_chars > 10, (
            f"Expected Chinese response, got: {response}"
        )


class TestMultiTurnConversation:
    """Test multi-turn conversation with context preservation."""
    
    async def test_context_memory_across_turns(self, deepseek_client):
        """Test that LLM remembers context from previous turns."""
        # Teacher persona that tracks student progress
        teacher_prompt = """ä½ æ˜¯Pythonå¯¼å¸ˆï¼Œæ­£åœ¨æ•™ä¸€ä½åˆå­¦è€…ã€‚
é‡è¦ï¼šè®°ä½å­¦ç”Ÿå·²ç»å­¦è¿‡çš„å†…å®¹ï¼Œä¸è¦é‡å¤æ•™å­¦ã€‚
å¦‚æœå­¦ç”Ÿè¡¨ç°å‡ºç†è§£ï¼Œå°±æ¨è¿›åˆ°ä¸‹ä¸€ä¸ªæ¦‚å¿µã€‚"""
        
        conversation = []
        
        # Turn 1: Student learns about variables
        q1 = "ä»€ä¹ˆæ˜¯å˜é‡ï¼Ÿ"
        conversation.append({"role": "user", "content": q1})
        r1 = await deepseek_client.chat(conversation, teacher_prompt)
        conversation.append({"role": "assistant", "content": r1})
        
        # Turn 2: Student shows understanding, asks follow-up
        q2 = "æˆ‘æ˜ç™½äº†ï¼Œå˜é‡å°±åƒç›’å­ã€‚é‚£åˆ—è¡¨æ˜¯ä»€ä¹ˆï¼Ÿ"
        conversation.append({"role": "user", "content": q2})
        r2 = await deepseek_client.chat(conversation, teacher_prompt)
        
        # Verify progression (should not re-explain variables)
        assert "ç›’å­" not in r2 or "å˜é‡" not in r2[:50], (
            "Should not re-explain variables, should progress to lists"
        )
    
    async def test_weekly_prompt_guided_learning(self, deepseek_client):
        """Test week-specific learning guidance."""
        # Week 2: Focus on hands-on practice
        week2_prompt = """è¿™æ˜¯ç¬¬2å‘¨ï¼šå®è·µç»ƒä¹ å‘¨ã€‚
è§„åˆ™ï¼š
1. ä¸ç»™å®Œæ•´ä»£ç ç¤ºä¾‹
2. åªæä¾›æ€è·¯å’Œä¼ªä»£ç 
3. é¼“åŠ±å­¦ç”Ÿè‡ªå·±å°è¯•å®ç°
4. å­¦ç”Ÿé—®ä»£ç æ—¶ï¼Œåé—®"ä½ è§‰å¾—ç¬¬ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼Ÿ"""
        
        conversation = [
            {"role": "user", "content": "å¸®æˆ‘å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°"}
        ]
        
        response = await deepseek_client.chat(conversation, week2_prompt)
        
        # Verify: no complete code, should have questions
        code_indicators = ["def ", "return ", "for ", "while "]
        has_code = any(indicator in response for indicator in code_indicators)
        
        # Allow some code snippets but not complete solution
        assert "?" in response or "ä½ è§‰å¾—" in response, (
            "Should guide with questions, not give complete answer"
        )


class TestWeekTransition:
    """Test prompt changes across week transitions."""
    
    async def test_different_week_different_style(self, deepseek_client):
        """Verify different week prompts produce different teaching styles."""
        question = "æ•™æˆ‘ Python å­—å…¸"
        
        # Week 1: Theory focused
        week1_prompt = "ç¬¬1å‘¨ï¼šç†è®ºæ¦‚å¿µå‘¨ã€‚è¯¦ç»†è§£é‡ŠåŸç†ï¼Œå¤šç”¨æ¯”å–»å’Œå®šä¹‰ã€‚"
        week1_response = await deepseek_client.chat(
            [{"role": "user", "content": question}],
            week1_prompt
        )
        
        # Week 3: Practice focused
        week3_prompt = "ç¬¬3å‘¨ï¼šå®æˆ˜ç»ƒä¹ å‘¨ã€‚ç»™å‡ºå®é™…ä¾‹å­å’Œç»ƒä¹ é¢˜ï¼Œå°‘è®²ç†è®ºã€‚"
        week3_response = await deepseek_client.chat(
            [{"role": "user", "content": question}],
            week3_prompt
        )
        
        # Verify different approaches
        week1_theory_words = ["å®šä¹‰", "æ¦‚å¿µ", "åŸç†", "æœ¬è´¨", "ç›¸å½“äº"]
        week3_practice_words = ["ä¾‹å­", "ç»ƒä¹ ", "è¯•è¯•", "å†™ä»£ç ", "å®ç°"]
        
        week1_theory_score = sum(1 for w in week1_theory_words if w in week1_response)
        week3_practice_score = sum(1 for w in week3_practice_words if w in week3_response)
        
        assert week1_theory_score >= 2, "Week 1 should focus on theory"
        assert week3_practice_score >= 2, "Week 3 should focus on practice"


class TestPromptRobustness:
    """Test edge cases with real LLM."""
    
    async def test_long_prompt_handling(self, deepseek_client):
        """Test system prompt with 1000+ characters."""
        long_prompt = "è¯¦ç»†è¯´æ˜ï¼š" + "è¿™æ˜¯é‡è¦è§„åˆ™ã€‚" * 200  # ~1400 chars
        
        response = await deepseek_client.chat(
            [{"role": "user", "content": "Hello"}],
            system_prompt=long_prompt
        )
        
        assert len(response) > 0
        assert isinstance(response, str)
    
    async def test_special_characters_in_prompt(self, deepseek_client):
        """Test system prompt with special characters."""
        special_prompt = """ç‰¹æ®Šå­—ç¬¦æµ‹è¯•ï¼š
- ä»£ç ï¼š`print("hello")`
- æ•°å­¦ï¼šxÂ² + yÂ² = zÂ²
- ç¬¦å·ï¼šâ†’ â† â†‘ â†“ âœ… âŒ
- Unicode: ğŸ Python ğŸš€"""
        
        response = await deepseek_client.chat(
            [{"role": "user", "content": "Say something"}],
            system_prompt=special_prompt
        )
        
        # Should not crash and return valid response
        assert len(response) > 0
