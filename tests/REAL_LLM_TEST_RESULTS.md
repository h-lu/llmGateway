# 真实 LLM 测试报告

**测试时间**: 2026-01-29  
**模型**: DeepSeek Chat (deepseek-chat)  
**总测试数**: 8  
**通过**: 5  
**失败**: 3  

---

## ✅ 通过的测试

### 1. `test_prompt_changes_llm_behavior`
- **目的**: 验证系统提示词真正改变 LLM 回复风格
- **方法**: A/B 测试，对比有无厨房比喻提示词
- **结果**: ✅ 通过
- **观察**: LLM 在使用厨房比喻提示词后，回复中确实包含了厨房相关词汇

### 2. `test_teaching_style_constraint`
- **目的**: 验证苏格拉底式教学风格约束
- **方法**: 强制使用提问引导学生思考
- **结果**: ✅ 通过
- **观察**: LLM 回复中包含至少3个问题

### 3. `test_language_constraint`
- **目的**: 验证系统提示词强制输出语言
- **方法**: 用英文提问，强制中文回答
- **结果**: ✅ 通过
- **观察**: LLM 用中文回复英文问题

### 4. `test_long_prompt_handling`
- **目的**: 测试 1000+ 字符的长提示词
- **方法**: 发送 1400 字符的系统提示词
- **结果**: ✅ 通过
- **观察**: API 正常处理长提示词

### 5. `test_special_characters_in_prompt`
- **目的**: 测试 Unicode 和特殊字符
- **方法**: 包含代码、数学符号、Emoji
- **结果**: ✅ 通过
- **观察**: API 正确处理特殊字符

---

## ❌ 失败的测试

### 1. `test_context_memory_across_turns`
- **目的**: 验证多轮对话上下文记忆
- **失败原因**: LLM 仍在使用"盒子"比喻来解释列表
- **分析**: 测试断言过于严格，LLM 确实在推进到列表概念，但使用了相似的比喻方式

### 2. `test_weekly_prompt_guided_learning`
- **目的**: 验证实战练习周不给出完整代码
- **失败原因**: LLM 提供了伪代码而非直接提问
- **分析**: 提示词要求"只提供思路和伪代码"，LLM 行为符合提示词，但测试期望更多提问

### 3. `test_different_week_different_style`
- **目的**: 验证不同周的提示词产生不同教学风格
- **失败原因**: 第3周回复中未检测到预期的"实战"关键词
- **分析**: LLM 可能使用了同义词或不同的表达方式

---

## 成本统计

- **总 API 调用**: 8 次
- **平均响应时间**: ~12 秒/测试
- **总耗时**: 97.28 秒
- **预估成本**: ~¥0.10 - 0.20

---

## 结论

1. **系统提示词确实有效**: 5/8 测试通过，证明提示词能影响 LLM 行为
2. **测试需要调整**: 失败测试的断言过于严格，需要考虑 LLM 输出的多样性
3. **DeepSeek API 稳定**: 无超时或连接错误

## 建议改进

1. 使用更灵活的断言（如语义相似度而非关键词匹配）
2. 增加重试机制处理 LLM 输出的随机性
3. 使用更明确的提示词来约束输出格式
